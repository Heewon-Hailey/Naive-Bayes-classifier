{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function opens a data file in csv, and transform it into a usable format \n",
    "import numpy as np\n",
    "def load_data():\n",
    "    data = open(\"student.csv\").read().strip()   \n",
    "    datalines = data.split(\"\\n\")\n",
    "    datafields = []\n",
    "    for line in datalines:\n",
    "        datafields.append(line.split(\",\"))\n",
    "        \n",
    "    return datafields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function splits a data set into a training set and hold-out test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy\n",
    "\n",
    "def split_data(datafields, random_number):\n",
    "    # separate data and labels\n",
    "    data_label = datafields[0]    \n",
    "    datafields = datafields[1:] \n",
    "    train, test = train_test_split(datafields, test_size = 0.1, random_state = random_number) \n",
    "    # convert the type from list to array\n",
    "    train = numpy.array(train)   \n",
    "    test = numpy.array(test)\n",
    "    data_label = numpy.array(data_label)\n",
    "    return data_label, train, test\n",
    "\n",
    "# Used 90% of data for training and 10% for testing to compare 10-fold cross validation later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This function builds a supervised NB model and returns prior/ likelihood dictionaries\n",
    "def train(data_label, train_set):\n",
    "    train = train_set\n",
    "    # a dictionary to count observed grades in training data\n",
    "    prior_count = {\"A+\":0, \"A\":0, \"B\":0, \"C\":0, \"D\":0, \"F\":0} \n",
    "    # a dictionary containing likelihood per each feature, per each grade\n",
    "    # it looks like {'grade1': {'att1': {'val1': pro1, 'val2': pro2},'att2': {'val1': pro1}, ....}\n",
    "    likelihood = {}       \n",
    "    for instance in train:\n",
    "        key = instance[-1]        # get a grade (will be used as a key) from training data to use as a key\n",
    "        prior_count[key] += 1     # count the number of grade in prior_count dictionary \n",
    "        if key not in likelihood.keys():      # create a new key(grade) when the grade is observed first time\n",
    "            likelihood[key] = {}\n",
    "        index = 0                             # index for feature label\n",
    "        for feature in instance[:-1]:         # count observed feature per each label in likelihood dictionary with its grade\n",
    "            label = data_label[index]\n",
    "            if label not in likelihood[key]:\n",
    "                likelihood[key][label]= {} \n",
    "            if feature not in likelihood[key][label]:\n",
    "                likelihood[key][label][feature]= 0\n",
    "            likelihood[key][label][feature] += 1\n",
    "            index += 1\n",
    "    total = len(train)          # count the total number of training instances (N)\n",
    "    prior = {}                  # a dictionary for prior \n",
    "    for grade in prior_count.keys():      # a dictionary containing the probability of each grade\n",
    "        prior[grade] = round(prior_count[grade]/total, 4)\n",
    "    \n",
    "    for grade in likelihood.keys():        # calculate conditional probability of each feature based on the counted number\n",
    "        for label in data_label[:-1]:      # get rid of the last label(grade) in data_label\n",
    "            for feature in likelihood[grade][label].keys():    # update the number of count with the probability\n",
    "                likelihood[grade][label][feature] = round(likelihood[grade][label][feature]/prior_count[grade], 4)\n",
    "    return prior, likelihood\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function predicts the grade for an instance or a set of instances, based on a trained model \n",
    "def predict(data_label, test_set, prior, likelihood):\n",
    "    test = test_set\n",
    "    epsilon = 0.0001             # use epsilon smoothing to avoid zero frequency; it is small enough cf. 1/649(0.0015)\n",
    "    predicted_set = []           # create a list containing the predicted grades\n",
    "    for instance in test:        # find the most possible grade for each instance\n",
    "        predicted = {'A+':0,'A':0,'B':0,'C':0,'D':0,'F':0}     # create a dictionary of each grade with its probability \n",
    "        predicted_max = 0             # initialise the maximum probability\n",
    "        predicted_grade = ''          # initialise the grade with the highest probability\n",
    "        for grade in predicted.keys():          # calculate the probability of each grade\n",
    "            probability = 1 \n",
    "            if not likelihood[grade]:           # substitute 0 with epsilon value\n",
    "                predicted[grade] *= epsilon\n",
    "            else:\n",
    "                index = 0                     # index for feature label\n",
    "                for feature in instance[:-1]:\n",
    "                    label = data_label[index]\n",
    "                    if feature not in likelihood[grade][label]:\n",
    "                        probability *= epsilon       # substitute '0' probability with epsilon value\n",
    "                    else: probability *= likelihood[grade][label][feature]\n",
    "                    index += 1\n",
    "            probability *= prior[grade]\n",
    "            predicted[grade] = probability\n",
    "            if predicted[grade] > predicted_max:       # update the maximum probability and the grade\n",
    "                predicted_max = predicted[grade]\n",
    "                predicted_grade = grade\n",
    "        predicted_set.append(predicted_grade)         # save the predicted grade \n",
    "    return predicted_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This function evaluates a set of predictions in terms of accuracy\n",
    "def evaluate(real_grades, predicted_set):              \n",
    "    correctly_predicted = 0                 \n",
    "    total_predicted = len(predicted_set)\n",
    "    for i in range(total_predicted):          # count correctly predicted grade\n",
    "        if real_grades[i] == predicted_set[i]:\n",
    "            correctly_predicted += 1\n",
    "    accuracy = (correctly_predicted/total_predicted)      # calculate the accuracy \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3692, 0.3692, 0.3231, 0.3538, 0.4769]\n",
      "0.37844\n",
      "Average accuracy : 37.8440 %\n",
      "Basic accuracy for comparison: 36.9200 %\n"
     ]
    }
   ],
   "source": [
    "# this code block is to estimate the Naive Bayes model using the hold-out strategy \n",
    "\n",
    "random_num_list = [1, 5, 33, 40, 100]        # create random numbers\n",
    "accuracy_list = []                           # initialise a list to save each result\n",
    "accuracy_sum = 0\n",
    "for num in random_num_list:\n",
    "    datafields = load_data()\n",
    "    data_label, train_set, test_set = split_data(datafields, num)\n",
    "    prior, likelihood = train(data_label, train_set)\n",
    "    predicted_set = predict(data_label, test_set, prior, likelihood)\n",
    "    real_grades = test_set[:,-1]         # get the actual grade from testing data\n",
    "    accuracy = round(evaluate(real_grades, predicted_set),4)\n",
    "    accuracy_list.append(accuracy)\n",
    "    accuracy_sum += accuracy\n",
    "print(accuracy_list)\n",
    "print(accuracy_sum/len(accuracy_list))\n",
    "print(\"Average accuracy : %.4f %%\" % (accuracy_sum*100/len(accuracy_list)))\n",
    "\n",
    "print(\"Basic accuracy for comparison: %.4f %%\" % (accuracy_list[0]*100))  # following questions are based on this model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision_dic : {'A+': 0, 'A': 0.1111, 'B': 0.25, 'C': 0.3333, 'D': 0.4118, 'F': 0.6154}\n",
      "Recall_dic : {'A+': 0, 'A': 0.1667, 'B': 0.1538, 'C': 0.4615, 'D': 0.4375, 'F': 0.6154}\n",
      "F1_dic : {'A+': 0, 'A': 0.1333, 'B': 0.1904, 'C': 0.3871, 'D': 0.4243, 'F': 0.6154}\n",
      "Average precision : 0.2869\n",
      "Average recall : 0.3058\n",
      "Average f1 : 0.2918\n",
      "Accuracy for comparison: 0.3692 \n"
     ]
    }
   ],
   "source": [
    "# This function calculates precision, recall and f1 score for the further analysis\n",
    "def getMetrics(real_grades, predicted_set):\n",
    "    grade_list = ['A+','A','B','C','D','F']\n",
    "    precision_dic = {'A+':0,'A':0,'B':0,'C':0,'D':0,'F':0}     # initialise dictionaries to save results\n",
    "    recall_dic = {'A+':0,'A':0,'B':0,'C':0,'D':0,'F':0}\n",
    "    f1_dic = {'A+':0,'A':0,'B':0,'C':0,'D':0,'F':0}\n",
    "    avg_precision, avg_recall, avg_f1 = 0, 0, 0\n",
    "    \n",
    "    # count true positive(TP), true negative(TN), false pasitive(FP), false negative(FN) per class           \n",
    "    for grade in grade_list:             \n",
    "        # initialise variables TP, TN, FP, FN\n",
    "        TP, TN, FP, FN = 0, 0, 0, 0  \n",
    "        for i in range(len(predicted_set)):\n",
    "            if predicted_set[i] == grade:\n",
    "                if real_grades[i] == grade:\n",
    "                    TP += 1\n",
    "                else: FP += 1\n",
    "            else:\n",
    "                if real_grades[i] == grade:\n",
    "                    FN += 1\n",
    "                else: TN += 1\n",
    "        if TP == 0 :             # when TP is 0, set the results as 0 to avoid zero division error\n",
    "            precision_dic[grade], recall_dic[grade], f1_dic[grade] = 0, 0, 0\n",
    "\n",
    "        else:     \n",
    "            precision_dic[grade] = round(TP/(TP + FP),4)\n",
    "            recall_dic[grade] = round(TP/(TP + FN),4)\n",
    "            f1_dic[grade] = round(2*(precision_dic[grade] * recall_dic[grade])\\\n",
    "                    /(precision_dic[grade] + recall_dic[grade]),4) \n",
    "            \n",
    "        avg_precision += precision_dic[grade]\n",
    "        avg_recall += recall_dic[grade]            \n",
    "        avg_f1 += f1_dic[grade]\n",
    "    avg_precision = round(avg_precision/len(grade_list),4)  # calculate macro average matrics\n",
    "    avg_recall = round(avg_recall/len(grade_list),4)\n",
    "    avg_f1 = round(avg_f1/len(grade_list),4)\n",
    "    print(\"Precision_dic :\", precision_dic)\n",
    "    print(\"Recall_dic :\", recall_dic)\n",
    "    print(\"F1_dic :\", f1_dic)\n",
    "    print(\"Average precision :\", avg_precision)\n",
    "    print(\"Average recall :\", avg_recall)\n",
    "    print(\"Average f1 :\", avg_f1)\n",
    "\n",
    "#execute the codes above\n",
    "datafields = load_data()\n",
    "data_label, train_set, test_set = split_data(datafields, 1)\n",
    "prior, likelihood = train(data_label, train_set)\n",
    "predicted_set = predict(data_label, test_set, prior, likelihood)\n",
    "real_grades = test_set[:,-1]      # get the actual grade from testing data\n",
    "accuracy = evaluate(real_grades, predicted_set)\n",
    "getMetrics(real_grades, predicted_set)\n",
    "print(\"Accuracy for comparison: %.4f \" % accuracy) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 iterations\n",
      "[0.3692, 0.2923, 0.4308, 0.3538, 0.3846, 0.3077, 0.3385, 0.2769, 0.3385, 0.3438]\n",
      "Average of accuracy : 0.3436\n"
     ]
    }
   ],
   "source": [
    "# This code block is to compare the outcomes of hold-out and cross-validation strategies\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# This function splits a data into n groups for cross-validation\n",
    "def split_n_data(datafields, random_number, n):    # takes the number of iteration as argument n\n",
    "    data_label = datafields[0]      # separate data and labels\n",
    "    datafields = datafields[1:] \n",
    "    test_sets, sets = [], []      # create lists to keep each train and test set\n",
    "    \n",
    "    while not n == 1:    \n",
    "        remaining, test = train_test_split(datafields, test_size = 1/n, random_state = random_number) \n",
    "        n -= 1\n",
    "        if(n == 1):              \n",
    "            test_sets += [test]\n",
    "            test_sets += [remaining]\n",
    "    \n",
    "        else: \n",
    "            test_sets += [test]\n",
    "            datafields = remaining\n",
    "    for i in range(len(test_sets)):\n",
    "        train_sets = []\n",
    "        test_set = test_sets[i]\n",
    "        for j in range(len(test_sets)):\n",
    "            if(j != i):\n",
    "                train_sets += test_sets[j]\n",
    "        sets += [[test_set, train_sets]]\n",
    "\n",
    "    return data_label, sets          # return data lists\n",
    "\n",
    "# the code below to get accuracy from cross-validation strategy\n",
    "datafields = load_data()\n",
    "data_label, sets = split_n_data(datafields, 1, 10)\n",
    "accuracy_list = []\n",
    "accuracy_sum = 0\n",
    "for [test_set, train_set] in sets:\n",
    "    test_set = np.array(test_set)\n",
    "    train_set = np.array(train_set)\n",
    "    prior, likelihood = train(data_label, train_set)\n",
    "    predicted_set = predict(data_label, test_set, prior, likelihood)\n",
    "    real_grades = test_set[:,-1]                # get the actual grade from testing data\n",
    "    accuracy = round(evaluate(real_grades, predicted_set),4)\n",
    "    accuracy_list.append(accuracy)\n",
    "    accuracy_sum += accuracy\n",
    "\n",
    "# the code below to get accuracy from hold-out strategy\n",
    "data_label, train_set, test_set = split_data(datafields, 1)\n",
    "prior, likelihood = train(data_label, train_set)\n",
    "predicted_set = predict(data_label, test_set, prior, likelihood)\n",
    "real_grades = test_set[:,-1]      # get the actual grade from testing data\n",
    "accuracy = evaluate(real_grades, predicted_set)\n",
    "\n",
    "print(\"10 iterations\")\n",
    "mean = (accuracy_sum/len(accuracy_list))\n",
    "print(accuracy_list)\n",
    "print(\"Average of accuracy : %.4f\" % mean)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
